{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_raw_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4ezF7jOFfaIE"},"source":["\r\n","**DATA SET**: Non Preprocessed\r\n","\r\n","**Model Bert**: bert-base-uncased\r\n","\r\n","**Fairness**: 4.159907527760487\r\n","\r\n","Best model was acquired in the 1st Epoch:\r\n","*  Training loss (TL):  0,664541470789616\r\n","*  Validation loss (VL):  0,593840423587227\r\n","*  |TL - VL| =  0,070701047202389\r\n","*  F1 Score (Macro): 0,790030958067818\r\n","\r\n","**Installing transformers**:\r\n"," \r\n","*  pip install transformers\r\n","\r\n","\r\n","**References**:\r\n","\r\n","\r\n","*   [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf?source=post_elevate_sequence_page---------------------------)\r\n","*   [BERT](https://huggingface.co/transformers/model_doc/bert.html) \r\n","*   [BERT For Sequence Classification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)\r\n","*   [Multi Class Text Classification With Deep Learning Using BERT](https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613)"]},{"cell_type":"code","metadata":{"id":"12ew8T8ZBqng"},"source":["import numpy as np\r\n","import pandas as pd\r\n","import random\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import f1_score\r\n","\r\n","import tensorflow as tf\r\n","\r\n","import torch\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from tqdm.notebook import tqdm\r\n","from transformers import BertTokenizer, BertForSequenceClassification\r\n","from transformers import AdamW, get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lm7xe2vceE8A"},"source":["#Methods use to evaluate the model\r\n","def f1_score_func(preds, labels):\r\n","    preds_flat = np.argmax(preds, axis=1).flatten()\r\n","    labels_flat = labels.flatten()\r\n","    return f1_score(labels_flat, preds_flat, average='macro')\r\n","\r\n","def accuracy_per_class(preds, labels):\r\n","\r\n","    label_dict_inverse = label_dict\r\n","    preds_flat = np.argmax(preds, axis=1).flatten()\r\n","    labels_flat = labels.flatten()\r\n","\r\n","    for label in np.unique(labels_flat):\r\n","        y_preds = preds_flat[labels_flat==label]\r\n","        y_true = labels_flat[labels_flat==label]\r\n","        print(f'Class: {label_dict_inverse[label]}')\r\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJihfBWNePSS"},"source":["#Setting GPU\r\n","if torch.cuda.is_available():    \r\n","    device = torch.device(\"cuda\")\r\n","else:\r\n","    print('No GPU available, using the CPU instead.')\r\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xk3JBVaCP78"},"source":["DATA_DEFI_PATH ='/content/gdrive/My Drive/defi-ia-insa-toulouse/'\r\n","LOGIT_OUTPUT_FILENAME = \"logits_bert_non_preprocessed_data.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFWHUqGABwTq"},"source":["non_preprocessed_train = pd.read_json(DATA_DEFI_PATH+\"train.json\").set_index('Id')\r\n","non_preprocessed_train['Category'] = pd.read_csv(DATA_DEFI_PATH+\"train_label.csv\").set_index('Id') \r\n"," \r\n","train_label = pd.DataFrame(non_preprocessed_train['Category'], columns=['Category'])\r\n","label_dict = pd.read_csv(DATA_DEFI_PATH+\"categories_string.csv\")['0'].to_dict()\r\n","\r\n","non_preprocessed_train['label_name'] = train_label.Category.replace(label_dict)\r\n","non_preprocessed_train['label'] =  train_label.Category"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLyoVQMpehQq"},"source":["seed_val = 23\r\n","random.seed(seed_val)\r\n","np.random.seed(seed_val)\r\n","torch.manual_seed(seed_val)\r\n","torch.cuda.manual_seed_all(seed_val)\r\n","tf.random.set_seed(seed_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66RRRozif6fS"},"source":["X_train, X_val, y_train, y_val = train_test_split(non_preprocessed_train.index.values, \r\n","                                                  non_preprocessed_train.label.values, \r\n","                                                  test_size=0.20, \r\n","                                                  random_state=seed_val, \r\n","                                                  stratify=non_preprocessed_train.label.values)\r\n","\r\n","non_preprocessed_train['data_type'] = ['not_set']*non_preprocessed_train.shape[0]\r\n","\r\n","non_preprocessed_train.loc[X_train, 'data_type'] = 'train'\r\n","non_preprocessed_train.loc[X_val, 'data_type'] = 'val'\r\n","df = non_preprocessed_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a-NEVRTUEjcH"},"source":["MAX_LENGTH = 128\r\n","batch_size = 6\r\n","epochs = 5\r\n","\r\n","# AdamW Optimizer hyperparameters\r\n","learning_rate = 2e-5\r\n","eps = 1e-8 \r\n","\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n","                                 \r\n","encoded_data_train = tokenizer.batch_encode_plus(\r\n","    df[df.data_type=='train'].description.values, \r\n","    add_special_tokens=True, \r\n","    return_attention_mask=True, \r\n","    pad_to_max_length=True, \r\n","    truncation=True,\r\n","    max_length=MAX_LENGTH, \r\n","    return_tensors='pt'\r\n",")\r\n","\r\n","encoded_data_val = tokenizer.batch_encode_plus(\r\n","    df[df.data_type=='val'].description.values, \r\n","    add_special_tokens=True, \r\n","    return_attention_mask=True, \r\n","    pad_to_max_length=True, \r\n","    truncation=True,\r\n","    max_length=MAX_LENGTH, \r\n","    return_tensors='pt'\r\n",")\r\n","\r\n","input_ids_train = encoded_data_train['input_ids']\r\n","attention_masks_train = encoded_data_train['attention_mask']\r\n","labels_train = torch.tensor(df[df.data_type=='train'].label.values)\r\n","\r\n","input_ids_val = encoded_data_val['input_ids']\r\n","attention_masks_val = encoded_data_val['attention_mask']\r\n","labels_val = torch.tensor(df[df.data_type=='val'].label.values)\r\n","\r\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\r\n","dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRHlgWIcgY9N"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\r\n","                                                      num_labels=len(label_dict),\r\n","                                                      output_attentions=False,\r\n","                                                      output_hidden_states=False)\r\n","\r\n","\r\n","dataloader_train = DataLoader(dataset_train, \r\n","                              sampler=RandomSampler(dataset_train), \r\n","                              batch_size=batch_size)\r\n","\r\n","dataloader_validation = DataLoader(dataset_val, \r\n","                                   sampler=SequentialSampler(dataset_val), \r\n","                                   batch_size=batch_size)\r\n","\r\n","\r\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\r\n","         \r\n","\r\n","\r\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nF1Vru7jClL_"},"source":["def evaluate(dataloader_val):\r\n","\r\n","    model.eval()\r\n","    model.to(device)\r\n","    loss_val_total = 0\r\n","    predictions, true_vals = [], []\r\n","    \r\n","    for batch in dataloader_val:\r\n","        \r\n","        batch = tuple(b.to(device) for b in batch)\r\n","        \r\n","        inputs = {'input_ids':      batch[0] ,\r\n","                  'attention_mask': batch[1] ,\r\n","                  'labels':         batch[2] ,\r\n","                 }\r\n","\r\n","        with torch.no_grad():        \r\n","            outputs = model(**inputs)\r\n","            \r\n","        loss = outputs[0]\r\n","        logits = outputs[1]\r\n","        loss_val_total += loss.item()\r\n","\r\n","        logits = logits.detach().cpu().numpy()\r\n","        label_ids = inputs['labels'].cpu().numpy()\r\n","        predictions.append(logits)\r\n","        true_vals.append(label_ids)\r\n","    \r\n","    loss_val_avg = loss_val_total/len(dataloader_val) \r\n","    \r\n","    predictions = np.concatenate(predictions, axis=0)\r\n","    true_vals = np.concatenate(true_vals, axis=0)\r\n","            \r\n","    return loss_val_avg, predictions, true_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ln1N5eTqGM7Z"},"source":["#TRAINING AND SAVING THE MODEL BY EPOCH\r\n","lista_loss_train = []\r\n","lista_loss_val = []\r\n","lista_f1_val = []    \r\n","for epoch in tqdm(range(1, epochs+1)):\r\n","    \r\n","    model.train()\r\n","    model.to(device)\r\n","    loss_train_total = 0\r\n","\r\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\r\n","    for batch in progress_bar:\r\n","\r\n","        model.zero_grad()\r\n","        \r\n","        batch = tuple(b.to(device) for b in batch)\r\n","                      \r\n","        inputs = {'input_ids':      batch[0] ,\r\n","                  'attention_mask': batch[1] ,\r\n","                  'labels':         batch[2] ,\r\n","                 }       \r\n","\r\n","        outputs = model(**inputs)\r\n","                 \r\n","        loss = outputs[0]\r\n","        loss_train_total += loss.item()\r\n","        loss.backward()\r\n","\r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","        optimizer.step()\r\n","        scheduler.step()\r\n","        \r\n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\r\n","         \r\n","         \r\n","    torch.save(model.state_dict(), f'./finetuned_BERT_NONPREPROCESSDATA_{epoch}.model')\r\n","        \r\n","    tqdm.write(f'\\nEpoch {epoch}')\r\n","    \r\n","    loss_train_avg = loss_train_total/len(dataloader_train)            \r\n","    tqdm.write(f'Training loss: {loss_train_avg}')\r\n","    \r\n","    val_loss, predictions, true_vals = evaluate(dataloader_validation)\r\n","    val_f1 = f1_score_func(predictions, true_vals)\r\n","    \r\n","    #Uncomment in case you want to generate the .csv file with all the ouputs\r\n","    #lista_loss_train.append(loss_train_avg)\r\n","    #lista_loss_val.append(val_loss)\r\n","    #lista_f1_val.append(val_f1)   \r\n","    \r\n","    #out = np.array([lista_loss_train,lista_loss_val,lista_f1_val])\r\n","    #out_df = pd.DataFrame(out.T, columns=['train_loss','val_loss','val_f1'])\r\n","    #out_df.to_csv('resultsByEpoch.csv',  index=False)\r\n","    \r\n","    tqdm.write(f'Validation loss: {val_loss}')\r\n","    tqdm.write(f'F1 Score (Macro): {val_f1}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YpLJGSWH7LLW"},"source":["## LOAD BEST MODEL AND FINAL TEST FILE\r\n","\r\n","\r\n","*   Storing logits in a csv file for  future use."]},{"cell_type":"code","metadata":{"id":"l0tb2aiph550"},"source":["BEST_MODEL_PATH = \"./data_volume/finetuned_BERT_NONPREPROCESSDATA_1.model\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmxHUunk7K0Q"},"source":["non_preprocessed_test = pd.read_json(DATA_DEFI_PATH+\"test.json\").set_index('Id')\r\n","label_dict = pd.read_csv(DATA_DEFI_PATH+\"categories_string.csv\")['0'].to_dict()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHdh-OFB868j","executionInfo":{"status":"ok","timestamp":1610226674636,"user_tz":-60,"elapsed":8716,"user":{"displayName":"Levi Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCzWliTNCNSl8P8iQ3is3Qa2F0Rb2Uni-H7MnfuEg=s64","userId":"12560956575705227821"}},"outputId":"7ec10da3-bd36-4977-da64-6c6398f2e27d"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\r\n","                                                      num_labels=len(label_dict),\r\n","                                                      output_attentions=False,\r\n","                                                      output_hidden_states=False)\r\n","\r\n","\r\n","model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=torch.device('cpu')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"a5VI3iD872tZ"},"source":["df  = non_preprocessed_test\r\n","\r\n","MAX_LENGTH = 128 \r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n","\r\n","encoded_data_test = tokenizer.batch_encode_plus(\r\n","    df.description.values, \r\n","    add_special_tokens=True, \r\n","    return_attention_mask=True, \r\n","    pad_to_max_length=True, \r\n","    truncation=True,\r\n","    max_length=MAX_LENGTH, \r\n","    return_tensors='pt'\r\n",")\r\n","\r\n","input_ids_test = encoded_data_test['input_ids']\r\n","attention_masks_test = encoded_data_test['attention_mask']\r\n","\r\n","dataset_test = TensorDataset(input_ids_test, attention_masks_test)\r\n","\r\n","dataloader_test = DataLoader(dataset_test, batch_size=1)\r\n","\r\n","\r\n","model.eval()\r\n","model.to(device)\r\n","predictions = []\r\n","\r\n","for batch in dataloader_test:\r\n","    #batch = tuple(b.to(device) for b in batch)\r\n","    batch = tuple(b.to(device) for b in batch)\r\n","\r\n","    inputs = {'input_ids':      batch[0] ,\r\n","              'attention_mask': batch[1]             \r\n","             }\r\n","    \r\n","    with torch.no_grad(): \r\n","        outputs = model(**inputs)\r\n","        \r\n","        logits = outputs[0]\r\n","        logits = logits.detach().cpu().numpy()\r\n","        predictions.append(logits)\r\n","        \r\n","# Saving Logits in a .csv File\r\n","predictions_logits = np.asarray(predictions)\r\n","predictions_logits = predictions_logits.reshape(1,54300,28)\r\n","predictions_logits = np.squeeze(predictions_logits)\r\n","data_logits = pd.DataFrame(predictions_logits)  \r\n","data_logits.to_csv(LOGIT_OUTPUT_FILENAME, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jWTK7nMi-s6"},"source":["#====================================================\r\n","# Generating output file for Kaggle submission\r\n","#predictions = np.concatenate(predictions, axis=0)\r\n","#preds_flat = np.argmax(predictions, axis=1).flatten()\r\n","#final_df = pd.DataFrame()\r\n","#final_df['Id'] = non_preprocessed_test.index\r\n","#final_df[\"Category\"] = preds_flat\r\n","#file = final_df[['Id','Category']]\r\n","#file.to_csv(\"/kaggle/working/bertModelNonPreprocessed.csv\", index=False)\r\n","#===================================================="],"execution_count":null,"outputs":[]}]}